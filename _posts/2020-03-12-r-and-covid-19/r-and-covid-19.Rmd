---
title: "COVID-19 in Europe"
description: |
  How bad the situation is. 
author:
  - name: Lorenzo Braschi
date: 03-13-2020
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
draft: false
bibliography: ../biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glue)
library(tidyverse)
library(rmarkdown)
library(plotly)
library(deSolve)
library(lubridate)

theme_set(theme_minimal()+
            theme(legend.position = "bottom", 
                  strip.text.y = element_text(angle = 0)))

countries_list <- c("Poland",
                    "Italy",
                    "Denmark",
                    "Switzerland",
                    "Sweden",
                    "Norway",
                    "Finland",
                    "Spain",
                    "France",
                    "Germany",
                    "Portugal",
                    "United Kingdom")

```

<!-- We'll be using the coronavirus dataset for the analyses. This analysis will be strongly influenced by Tim Church's analyses[@churches2020covid19rpart1;@churches2020covid19rpart2], but applied to Poland.  -->

# Get the data

For data we will use the [coronavirus](https://ramikrispin.github.io/coronavirus/articles/intro_coronavirus_dataset.html) dataset, which is a conveniently tidied table based on the Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE). The original data is available at the [CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19) repository if you prefer to do the cleanup yourself. 

<aside>
[![](https://ramikrispin.github.io/coronavirus/reference/figures/coronavirus.png)](https://ramikrispin.github.io/coronavirus/)
</aside>


The coronavirus package is published to CRAN, but it is updated on GitHub on a daily basis. To get the latest version, we run: 

```{r}
remotes::install_github("RamiKrispin/coronavirus")
```

Another dataset that is available is the nCov2019 package[@Wu2020-et] available from [their GitHub site](https://github.com/GuangchuangYu/nCov2019). They provide a useful [vignette](https://guangchuangyu.github.io/nCov2019/) as well. This package does more than get data, it also provides function utilities for mapping and plotting the cases. We can obtain the latest version by running:

```{r}
remotes::install_github("GuangchuangYu/nCov2019")
```



## The coronavirus dataset

We can have a look at the data. As we see, each row is a particular _record_ with the number of cases, confirmed, dead or recovered, for a given region. Geographical coordinates are added for map plots. 

```{r, layout = "l-body-outset"}
library(coronavirus)
# View the data
paged_table(coronavirus)

```

Each row of this table gives the number of cases reported per each day and region between `r format(min(coronavirus$date), "%B %d %Y")` and `r format(max(coronavirus$date), "%B %d %Y")`. 

## The nCov2019 dataset 

```{r}
library(nCov2019)

all.ncov <- load_nCov2019(lang = "en") %>% .['global'] %>% tbl_df()

# Filter ncov for the same coutries we're interested in: 
all.ncov %>%
  filter(country %in% countries_list) %>% 
  paged_table(all.ncov)
```

This dataset is also collected from the GitHub repo, and contains data from `r format(min(all.ncov$time), "%B %d %Y")` to `r format(max(all.ncov$time), "%B %d %Y")`. `r ifelse(max(all.ncov$time) >= max(coronavirus$date), "The all.ncov2019 has more recent data than the coronavirus dataset, and thus", "Both all.ncov2019 and the coronavirus datasets have up to date data, however")` we will use the all.ncov2019 dataset as it is usually updated faster. 

# Exploratory Analysis

```{r, layout = "l-body-outset"}
# Summarise per country and add a cumulative count
all.corona <- coronavirus %>% 
  select(-Lat, -Long) %>% 
  rename(country = Country.Region) %>% 
  group_by(country, date, type) %>% 
  summarise(cases = sum(cases)) %>% 
  arrange(country, date, type) %>% 
  group_by(country, type) %>% 
  # Add a cumulative sum of cases
  mutate(cumcases = cumsum(cases)) %>% 
  arrange(desc(cumcases))

# Visualise all the countries caces data
paged_table(all.corona)
```

We immediately see that in China the total number of confirmed cases is still high, but the new cases are extremely low. If we look instead at the data from nCov2019, we get very similar numbers (but not exactly identical, as it is expected). 

```{r}
all.ncov %>% 
  arrange(desc(cum_confirm)) %>% 
  paged_table()
```

A limitation of the nCov2019 dataset is that it does not give us the number of cases, but we can easily work around that by simply getting the difference from one day to the previous one:

```{r}
all.ncov <- all.ncov %>% 
  arrange(country, time) %>% 
  group_by(country) %>% 
  mutate(
    cases_confirmed = cum_confirm - lag(cum_confirm), 
    cases_recovered = cum_heal - lag(cum_heal), 
    cases_death = cum_dead - lag(cum_dead)
  ) %>% 
  # Remove empty rows; just removing if empty confirm should be enough
  rename(cum_confirmed = cum_confirm, 
         cum_recovered = cum_heal, 
         cum_death = cum_dead,
         date = time) %>% 
  filter(!is.na(cases_confirmed))
```

The coronavirus dataset from John Hopkins and the data from the nCov2019 packages come from different sources, that can be updated at different times and with different degrees of accuracy. Therefore it would be a good idea to run a comparison of both datasets, to see if there any major discrepancies between the datasets. 

<aside>
It is important to acknowledge that both datasets are actually collections of collections of datasets, from sources in many languages across many hospitals and centers. The mere fact that we have such vast amounts of data quickly available is mindboggling. 
</aside>

Comparing the coronavirus and the nCov2019 datasets require a little bit of data manipulation as they are in different formats. 

```{r, layout = "l-page", fig.height = 6}
long.ncov <- inner_join(
  all.ncov %>% 
    select(-starts_with("cases_")) %>% 
    pivot_longer(cum_confirmed:cum_death, 
                 names_to = "type",
                 names_prefix = "cum_",
                 values_to = "cumcases"), 
  all.ncov %>% 
    select(-starts_with("cum_")) %>% 
    pivot_longer(cases_confirmed:cases_death, 
                 names_to = "type", 
                 names_prefix = "cases_",
                 values_to = "cases"), 
  by = c("date", "country", "type")
)


joined <- full_join(all.corona, 
                    long.ncov, 
                    by = c("date", "country", "type"), 
                    suffix = c("_coronavirus", "_nCov2019")) %>% 
  pivot_longer(cases_coronavirus:cases_nCov2019,
               names_to = "dataset") %>% 
  separate(dataset, c("class", "dataset"), "_")


# We get examples for our data of interest
joined %>% 
  filter(country %in% countries_list, 
         type == "confirmed", 
         class == "cumcases",
         date >= "2020-03-01") %>% 
  ggplot(aes(date, value, color = dataset))+
  geom_line()+
  geom_point()+
  facet_wrap(country~., scales = "free")+
  scale_color_viridis_d()+
  labs(title = "A comparison between the coronavirs (John Hopkins) and the nCov2019 (China) COVID-19 datasets", 
       subtitle = "Cumulative cases comparison.Y axes not to scale.", 
       caption = "Data limited from March 1 to enhance readabilty")

```

The two datasets are in remarkable agreement as to the number of cumulative cases, and the mior discrepancies won't impact much our modelling. As we have pointed out above, we will use the nCov2019 dataset as it is usually updated faster. We will have first a look at the nCov2019, restricting our analysis to those european countries for which we have a reasonably large number of cases. 

```{r, echo = FALSE, results='markup'}
glue::glue("{1:length(countries_list)}. {sort(countries_list)}")
```


We will focus this analysis on this `r length(countries_list)` european countries only. We can get the table of the new confirmed cases today and the total confirmed cases up to date:

```{r}
countries <- long.ncov %>% 
  filter(country %in% countries_list) 

countries %>% 
  ungroup %>% 
  filter(date == max(date), 
         type == "confirmed") %>% 
  select(country, cases, cumcases) %>% 
  arrange(desc(cumcases)) %>% 
  paged_table()

```

We clearly see that __Italy__ is still the most gravely affected EU country by far with  
`r countries %>% filter(country == "Italy", date == max(date)) %>% pluck("cumcases", 1)` accumulated cases as of `r countries %>% filter(country == "Italy", date == max(date)) %>% pluck("date", 1) %>% format("%B %d")`. By this date, `r countries %>% filter(country == "Italy", date == max(date)) %>% pluck("cumcases", 2)` patients have died already. The number of new confirmed cases _yesterday_ (last report) in Italy alone is `r countries %>% filter(country == "Italy", date == max(date)) %>% pluck("cases", 1)`. 

__Spain__ is also severely affected. The country has `r countries %>% filter(country == "Spain", date == max(date)) %>% pluck("cumcases", 1)` patients accumulated so far, with `r countries %>% filter(country == "Spain", date == max(date)) %>% pluck("cases", 1)` new confirmed cases as of `r countries %>% filter(country == "Spain", date == max(date)) %>% pluck("date", 1) %>% format("%B %d")`. So far there have been `r countries %>% filter(country == "Spain", date == max(date)) %>% pluck("cumcases", 2)` casualties due to the virus.

My country of residence, __Poland__ has it comparably much better. There are `r countries %>% filter(country == "Poland", date == max(date)) %>% pluck("cumcases", 1)` confirmated cases so far, with `r countries %>% filter(country == "Poland", date == max(date)) %>% pluck("cases", 1)` new cases as of `r countries %>% filter(country == "Poland", date == max(date)) %>% pluck("date", 1) %>% format("%B %d")`. The number of casualties is still low, but already `r countries %>% filter(country == "Poland", date == max(date)) %>% pluck("cumcases", 2)` people have already died due to the virus. 

We can plot on the number of cumulative confirmed cases up to each day: 

```{r, layout = "l-page", fig.height = 6}
(cumconfplot <- countries %>% 
   filter(date >= "2020-01-01") %>% 
   ggplot(aes(date, cumcases, color = country, linetype = type)) + 
   facet_grid(type~., scales = "free")+
   geom_line()+
   geom_point()+
   theme(legend.position = "bottom", 
         strip.text.y = element_text(angle = 0))+
   scale_color_viridis_d()+
   labs(
     x = "Date of report",
     y = "",
     title = "Cases of COVID-19 per country", 
     subtitle = "Note: Y axes not to scale", 
     caption = glue("Data last updated on {format(Sys.Date()-1, '%m-%d-%Y')}"))
)
# ggplotly(cumconfplot)
```

This is definitely not loooking good. Another way of looking at this is to see how many new cases are being confirmed each day, or the __daily incremental incidence__. This is referred to as the _epidemic curve_, which is usually plotted as a bar chart: 

```{r, layout = "l-body-outset", fig.height=10}
(epicurves <- countries %>% 
   filter(type == "confirmed", 
          date >= "2020-02-20") %>% 
   ggplot(aes(date, cases))+
   geom_bar(stat = "identity", show.legend = FALSE, fill = "steelblue")+
   # geom_bar_interactive(stat = "identity", show.legend = FALSE, fill = "steelblue", 
   #                      aes(tooltip = cases, data_id = country))+
   facet_grid(country~., scales = "free")+
   theme(legend.position = "", 
         strip.text.y = element_text(angle = 0))+
   scale_fill_viridis_d()+
   labs(title = "Epidemic curve for each country",
        subitle = "Number of new cases per country")+
   scale_x_date("Date of report")+
   scale_y_continuous("Number of cases")
)
```

This looks bad. The largest number of confirmed cases are produced by the lates date of reporting, which means that the epidemic is far from being controlled. (Also we see we what seems to be a common pattern of missing data around March 12, as it seems unlikely there were suddenly no or little number of cases on that date. It is also possible that the data from March 13 reflect a dump of the 12th and 13th combined). 

In comparison, lets' have a look at how the situation looks in South Korea. South Korea was one of the countries that was heavily stuck by the SARS 2003 and 2009 epidemics, and learned how to deal with such cases. For most of the time, they had the situation under control, until a super spreader (called Patient-31) in the Daegu province managed to infect a large number of people in a short time, exploding the number of cases. 

<aside>
More about patient 31 here: [The Korean clusters:
How coronavirus cases exploded in South Korean churches and hospitals]( https://graphics.reuters.com/CHINA-HEALTH-SOUTHKOREA-CLUSTERS/0100B5G33SB/index.html)
</aside>

Since then however, the situation seems to back to relative control as the number of new cases is quickly dropping down. 

```{r}
(s.korea <- long.ncov %>% 
   filter(country == "South Korea",
          type == "confirmed",
          date >= "2020-02-20") %>%  
   ggplot(aes(date, cases))+
   geom_bar(stat = "identity", fill = "steelblue")+
   theme(legend.position = "", 
         strip.text.y = element_text(angle = 0))+
   scale_fill_viridis_d()+
   labs(title = "Epidemic curve for South Korea")+
   scale_x_date("Date of report")+
   scale_y_continuous("Number of cases")
 
)
```

So it seems that the number of confirmed cases in South Korea is really coming down, although we would have to check the individual provinces to understand the situation there better.  

Lastly, we can have a look at the situation in China: 

```{r}
(s.korea <- long.ncov %>% 
   filter(country == "China",
          type == "confirmed",
          date >= "2020-02-20") %>%  
   ggplot(aes(date, cases))+
   geom_bar(stat = "identity", fill = "steelblue")+
   theme(legend.position = "", 
         strip.text.y = element_text(angle = 0))+
   scale_fill_viridis_d()+
   labs(title = "Epidemic curve for China")+
   scale_x_date("Date of report")+
   scale_y_continuous("Number of cases")
 
)
```



# Modelling

## SIR models

Following Tim Churches' @churches2020covid19rpart1 approach, we could try to fit a SIR model to our data, in which the outbreak is modelled as a series of compartments (Susceptible, Infected and Recovered), with parameters $\beta$ and $\gamma$ to model the rate of transfer from susceptible to infected and from infected to recovered, respectively. The differential equations given by Churches are simple enough: 

$$
\frac{dS}{dt} = - \frac{\beta IS}{N}
$$

$$
\frac{dI}{dt} = \frac{\beta IS}{N} - \gamma I
$$

$$
\frac{dR}{dt} = \gamma I
$$

Then there is the matter of solving this system of differential equations by optmising the parameters $\beta$ and $\gamma$, either by minimising the Residual Sum of Squares or by Maximum Likelihood (or some Bayesian cleverness). The function `deSolve::ode()` is used for solving ordinary differential equations and the function `base::optim()` finds the parameters values that minimise the residual sum of squares.

$$
RSS(\beta, \gamma) = \sum_t (I(t) - \hat I(t))^2
$$

Most importantly, we want to model not just a single country, but a list of them; it would be of course highly inefficient to basically repeat the same lines of code over and over, replacing the country name at every step. So we need to wrap the modelling step in a function. Then we will apply this function via `purrr::map()` to a nested dataset. Let's go step by step: 

First, the SIR model needs a Susceptible parameter. This is usually taken to mean the population of a given country. Our datasets do not incorporate this information; however, the dataset `population` in base R contains population values for 219 countries/states up to 2013. We can use that to get the number of Susceptible people, as the population for the countries of interest hasn't changed much since 2013. 

```{r, results = "hide"}

# We need the population. default dataset "population" in R has data up to 2013. Outdated but it's the best we have. 
countries.nest <- countries %>% 
  nest() %>% 
  left_join(
    population %>% 
      mutate(
        country = ifelse(country == "United Kingdom of Great Britain and Northern Ireland",
                         "United Kingdom", country)) %>% 
      filter(year == max(year)) %>% select(-year), 
    by = "country"
  ) %>% 
  select(country, population, data) %>% 
  rename(N = population)



# put the daily cumulative incidence numbers for Hubei from
# 15th Jan to 30th Jan into a vector called Infected

SIR_model <- function(country, data, N){
  
  # data <- data[[1]]
  
  sir_start_date <- data %>% 
    pluck("date") %>% 
    min(.)
  
  Infected <- data %>% 
    filter(type == "confirmed") %>% 
    pull(cumcases)
  
  Day <- 1:(length(Infected))
  
  init <- c(S = N - Infected[1], I = Infected[1], R = 0)

  SIR <- function(time, state, parameters) {
    par <- as.list(c(state, parameters))
    with(par, {
      dS <- -beta * I * S/N
      dI <- beta * I * S/N - gamma * I
      dR <- gamma * I
      list(c(dS, dI, dR))
    })
  }
  
  RSS <- function(parameters) {
    names(parameters) <- c("beta", "gamma")
    out <- ode(y = init, times = Day, func = SIR, parms = parameters)
    fit <- out[, 3]
    sum((Infected - fit)^2)
  }
  
  Opt <- optim(c(0.5, 0.5), 
               RSS, 
               method = "L-BFGS-B", 
               lower = c(0,0), 
               upper = c(1, 1)
  )
  
  Opt_par <- setNames(Opt$par, c("beta", "gamma"))
  
  t <- 1:as.integer(today() - sir_start_date)
  
  fitted_cumulative_incidence <- data.frame(
    ode(y = init, times = t, func = SIR, parms = Opt_par)
  )

  fitted_cumulative_incidence <- fitted_cumulative_incidence %>% 
    mutate(date = ymd(sir_start_date) + days(t - 1), 
           country = country, 
           param.beta = Opt_par["beta"] %>% unname, 
           param.gamma = Opt_par["gamma"] %>% unname, 
           r0 = param.beta/param.gamma) %>% 
    left_join(
      data %>% 
        filter(type == "confirmed") %>% 
        select(date, cumcases, cases), 
      by = c("date")) 
    
  return(fitted_cumulative_incidence)
}

countries.nest <- countries.nest %>% 
  mutate(
    sirmodel = pmap(list(country, data, N), SIR_model)
  ) 

countries.nest <- countries.nest %>% 
  mutate(
    sirplot = map2(
      sirmodel,
      country,
      ~ggplot(data = .x, aes(date))+
        geom_line(aes(y = I), colour = "red") +
        geom_point(aes(y = cumcases), colour = "orange") +
        labs(y = "Cumulative incidence",
             title = glue("COVID-19 fitted vs observed cumulative incidence, {.y}"),
             caption = "(red=fitted incidence from SIR model, orange=observed incidence)",
             subtitle = glue("Basic Reproduction Number R0 = {round(.x$r0[1], 3)}"))
    )
  )


countries.nest$sirplot


```


## Predictions with the SIR models




~~However, one limitation of the model (that Churches points out) is that the SIR (and extended models such as SEIR) model is limited due to a number of assumptions. One particular weak assumption is that the number of cases reported (ie, lab confirmed cases) represent _all the infectious cases_. This is very likely not true, as typically only a proportion of actual cases are tested, a proportion which is named _ascertainment rate_. Moreover, this ascertainment rate is bound to change, due to the changes in policy the authorities are implementing. One can try to _estimate_ or simulate this ascertainment rate by applying weights on the incidence data.~~ 





## Corrections {.appendix}

Any mistakes or suggested changes please [open an issue](https://github.com/Lornebradia/r-experiments/issues/new) on the source repository. 


## Links {.appendix}

